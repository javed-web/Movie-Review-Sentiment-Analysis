{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53569,"databundleVersionId":5834979,"sourceType":"competition"}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.021774,"end_time":"2023-06-10T16:54:04.579057","exception":false,"start_time":"2023-06-10T16:54:04.557283","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nimport re, string, ast\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.ensemble import GradientBoostingClassifier\n#from sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading, Exploration","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/train.csv\")\ndata2 = pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/movies.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first few rows of the dataset\ndata1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data1.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check data types\nprint(data1.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the duplicates in data2\ndata2.drop_duplicates(subset=['movieid'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2.shape # 16,854 Duplicates were dropped","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first few rows of the dataset\ndata2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check data types\nprint(data2.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the data based on the movie ID\ndata = pd.merge(data1, data2, on='movieid', how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop unnecessary columns\ndata = data.drop(['rating', 'originalLanguage', 'runtimeMinutes','title', 'movieid', 'reviewerName', 'soundType', 'distributor', 'boxOffice', 'releaseDateStreaming', 'releaseDateTheaters'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check data types and missing values\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = data.isnull().sum()\nprint(missing_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill the missing values\ndata['reviewText'].fillna(value='', inplace=True)\ndata['genre'].fillna(value='', inplace=True)\ndata['ratingContents'] = data['ratingContents'].fillna(pd.Series([[]] * len(data)))\ndata['audienceScore'] = data['audienceScore'].fillna(0.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Countplot of the target variable\nplt.figure(figsize=(6, 4))\nsns.countplot(x='sentiment', data=data)\nplt.title('Distribution of Target Variable')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Countplot of a categorical feature against the target\nplt.figure(figsize=(8, 6))\nsns.countplot(x='isFrequentReviewer', hue='sentiment', data=data)\nplt.title('Countplot of Categorical Feature by Target')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Function to preprocess the reviewtext\ndef preprocess_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', '', text)\n\n    # Remove special characters (except for spaces)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n\n    # Remove non-ASCII characters\n    text = text.encode('ascii', 'ignore').decode('utf-8')\n\n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n\n    # Tokenize the text into individual words or tokens\n    tokens = text.split()\n\n    # Remove short words (less than 3 characters)\n    tokens = [token for token in tokens if len(token) > 2]\n\n    # Convert tokens back to text\n    text = ' '.join(tokens)\n\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"# Preprocessing: Apply text preprocessing to 'reviewText'\ndata['reviewText'] = data['reviewText'].apply(preprocess_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a pipeline for text feature 'reviewText'\ntext_preprocessor = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=15000))\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding the categorical features","metadata":{}},{"cell_type":"code","source":"# Create a pipeline for Categorical features 'genre, ratingContents', 'director'\ncategorical_preprocessor = Pipeline([\n    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the boolean and text data into Numerical 0's and 1's\ndata['isFrequentReviewer'] = data['isFrequentReviewer'].map({True: 1, False: 0})\ndata['sentiment'] = data['sentiment'].map({'POSITIVE': 1, 'NEGATIVE': 0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling","metadata":{}},{"cell_type":"code","source":"# Create a pipeline for Numerical feature 'audienceScore'\nnumerical_preprocessor = Pipeline([\n    ('scaler', StandardScaler())\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the string representation of lists into actual lists\ndata['ratingContents'] = data['ratingContents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n# Preprocessing: Remove square brackets and convert each list into a string\ndata['ratingContents'] = data['ratingContents'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Column Transformer","metadata":{}},{"cell_type":"code","source":"# Create a column transformer to apply the pipelines to the respective columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', text_preprocessor, 'reviewText'),\n        ('categorical', categorical_preprocessor, ['genre', 'ratingContents', 'director']),\n        ('numerical', numerical_preprocessor, ['audienceScore'])\n    ],\n    remainder='drop'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = data.isnull().sum()\nprint(missing_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into features (X) and target (y)\nX = data.drop('sentiment', axis=1)\ny = data['sentiment'].values\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#Gradient Boost: 0.6898\n#XG Boost: 0.7552, 0.7962 features: 15000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression(random_state=42)\n#model = xgb.XGBClassifier(random_state=42)\n#model = RandomForestClassifier(random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross Validation and Hyper Parameter Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define the hyperparameter grid\n'''param_grid = {\n    'classifier__n_estimators': [100, 200, 300, 400],\n    'classifier__max_depth': [None, 10, 20, 30]\n}'''\n\n# Define the hyperparameter grid for LogisticRegression\nparam_grid = {\n    'classifier__C': [0.01, 0.1, 1.0, 10.0],  # Regularization parameter\n    'classifier__max_iter': [800, 1000, 1200],   # Maximum number of iterations\n    'classifier__penalty': ['l1', 'l2'],       # Regularization type\n    'classifier__solver': ['liblinear'],       # Solver for optimization\n}\n\n# Create a new pipeline with the ColumnTransformer, LogisticRegression, and GridSearchCV\npipeline = Pipeline([\n    ('pipeline', preprocessor),\n    ('classifier', model)\n])\n\n# Initialize GridSearchCV with the pipeline and hyperparameter grid\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_weighted')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Fit the GridSearchCV to the training data\ngrid_search.fit(X_train, y_train)\n\n# Get the best model from the GridSearchCV\nbest_model = grid_search.best_estimator_\n\n# Refit the best model on the entire training dataset\nbest_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = best_model.predict(X_test)\n\n# Calculate the F1 score on the test data\nf1 = f1_score(y_test, y_pred, average='weighted')\nprint(\"Best XGB F1 Score:\", f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# New test data","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/test.csv')\n\n# Merge the data based on the movie ID\ntest = pd.merge(test, data2, on='movieid', how='left')\n\n# Drop unnecessary columns\ntest = test.drop(['rating', 'originalLanguage', 'runtimeMinutes','title', 'movieid', 'reviewerName', 'soundType', 'distributor', 'boxOffice', 'releaseDateStreaming', 'releaseDateTheaters'], axis=1)\ntest.rename(columns={'isTopCritic': 'isFrequentReviewer'}, inplace=True)\n\nmissing_values = test.isnull().sum()\nprint(missing_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill the missing values\ntest['reviewText'].fillna(value='', inplace=True)\ntest['genre'].fillna(value='', inplace=True)\ntest['audienceScore'] = test['audienceScore'].fillna(0.0)\ntest['ratingContents'] = test['ratingContents'].fillna(pd.Series([[]] * len(test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = test.isnull().sum()\nprint(missing_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['reviewText'] = test['reviewText'].apply(preprocess_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the string representation of lists into actual lists\ntest['ratingContents'] = test['ratingContents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n# Preprocessing: Remove square brackets and convert each list into a string\ntest['ratingContents'] = test['ratingContents'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\ntest['isFrequentReviewer'] = test['isFrequentReviewer'].map({True: 1, False: 0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform the new test data using the preprocessor within the best model\nX_test_new = best_model.named_steps['pipeline'].transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test data using the best model\ny_pred_test = best_model.named_steps['classifier'].predict(X_test_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate the classification report\nclass_names = ['POSITIVE', 'NEGATIVE']\nreport = classification_report(y_test, y_pred, target_names=class_names)\n\nprint(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Create a heatmap of the confusion matrix using seaborn\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Comparision","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# f1 scores\nf1_scores = [0.80347, 0.8001, 0.7549]\n\n# Model names\nmodel_names = ['Logistic Regression', 'XGBoost', 'Random Forest']\n\n# Set style using Seaborn\nsns.set(style='whitegrid')\n\n# Create the bar plot\nplt.figure(figsize=(10, 6))\nbars = plt.barh(model_names, f1_scores, color='skyblue')\n\n# Add data labels to the bars\nfor bar in bars:\n    width = bar.get_width()\n    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width:.3f}', ha='left', va='center')\n\n# Remove spines\nsns.despine()\n\n# Add labels and title\nplt.xlabel('f1 Score')\nplt.ylabel('Model')\nplt.title('f1 Scores of classification Models')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"1. Logistic Regression after performing some Hyperparameter tuning gave best results among other models.\n\n2. Overall Model Performance: The logistic regression model with hyperparameter tuning achieved an accuracy of 80% on the test data. This means that 80% of the predictions made by the model were correct.\n\n3. Precision and Recall: The precision for the \"POSITIVE\" class is 0.75, indicating that when the model predicts a review as \"POSITIVE,\" it is correct 75% of the time. The recall for the \"POSITIVE\" class is 0.60, suggesting that the model identifies only 60% of the actual \"POSITIVE\" reviews. Similarly, for the \"NEGATIVE\" class, the precision is 0.82 (correct 82% of the time) and the recall is 0.90 (identifies 90% of actual \"NEGATIVE\" reviews).\n\n4. F1-Score: The F1-score is a metric that balances precision and recall. The weighted average F1-score is 0.80, which is a measure of the model's overall performance across both classes.","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Save the predictions to a CSV file\nsubmission = pd.DataFrame({\n    'id': range(len(test)),  # Create the 'id' column starting with 0\n    'sentiment': y_pred_test  # Add the predicted sentiment values\n})\nsubmission['sentiment'] = submission['sentiment'].map({1: 'POSITIVE', 0: 'NEGATIVE'})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}